{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini project 1: air quality in U.S. cities\n",
    "\n",
    "In a way, this project is simple: you are given some data on air quality in U.S. metropolitan areas over time together with several questions of interest, and your objective is to answer the questions.\n",
    "\n",
    "However, unlike the homeworks and labs, there is no explicit instruction provided about *how* to answer the questions or where exactly to begin. Thus, you will need to discern for yourself how to manipulate and summarize the data in order to answer the questions of interest, and you will need to write your own codes from scratch to obtain results. It is recommended that you examine the data, consider the questions, and plan a rough approach before you begin doing any computations.\n",
    "\n",
    "You have some latitude for creativity: **although there are accurate answers to each question** -- namely, those that are consistent with the data -- **there is no singularly correct answer**. Most students will perform similar operations and obtain similar answers, but there's no specific result that must be considered to answer the questions accurately. As a result, your approaches and answers may differ from those of your classmates. If you choose to discuss your work with others, you may even find that disagreements prove to be fertile learning opportunities.\n",
    "\n",
    "The questions can be answered using computing skills taught in class so far and basic internet searches for domain background; for this project, you may wish to refer to HW1 and Lab1 for code examples and the [EPA website on PM pollution](https://www.epa.gov/pm-pollution) for background. However, you are also encouraged to refer to external resources (package documentation, vignettes, stackexchange, internet searches, etc.) as needed -- this may be an especially good idea if you find yourself thinking, 'it would be really handy to do X, but I haven't seen that in class anywhere'.\n",
    "\n",
    "The broader goal of these mini projects is to cultivate your problem-solving ability in an unstructured setting. Your work will be evaluated based on the following:\n",
    "- choice of method(s) used to answer questions;\n",
    "- clarity of presentation;\n",
    "- code style and documentation.\n",
    "\n",
    "Please write up your results separately from your codes; codes should be included at the end of the notebook.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I\n",
    "\n",
    "Merge the city information with the air quality data and tidy the dataset (see notes below). Write a one- to two-paragraph description of the data.\n",
    "\n",
    "In your description, answer the following questions:\n",
    "\n",
    "- What is a CBSA (the geographic unit of measurement)?\n",
    "- How many CBSA's are included in the data?\n",
    "- In how many states and territories do the CBSA's reside? (*Hint: `str.split()`*)\n",
    "- In which years were data values recorded?\n",
    "- How many observations are recorded?\n",
    "- How many variables are measured?\n",
    "- Which variables are non-missing most of the time (*i.e.*, in at least 50% of instances)?\n",
    "- What is PM 2.5 and why is it important?\n",
    "- What are the basic statistical properties of the variable(s) of interest?\n",
    "\n",
    "Please write your description in narrative fashion; _**please do not list answers to the questions above one by one**_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Air quality data\n",
    "*The Air quality data defines CBSA as Core Based Statistical Areas and reports observations from 351 different CBSA's which are a part of 52 States and Territories. There is a total of 1134 observations recorded from 2000-2019. The variables that are measured are the Pollutants, the specific reports on that pollutant, the number of trend sites, the year, and the CBSA. If you are considering all types of pollutants for all CBSA's then PM2.5 is the only pollutant that is non-missing more than 50% of the time. The variables that are non-missing most of the time are the year, CBSA, Trend Statistic, # of Trend Sites. It seems that the pollutant that has the least percentage of missing measurements is PM 2.5. This pollutant refers to tiny particles in the air that are 2.5 micrometers or smaller in diameter. PM 2.5 is important because it is dangerous to people who are exposed to high levels of it overtime.*\n",
    "\n",
    "*For **PM2.5** __Weighted Annual Mean__, The Mean = 10.137 and Variance = 10.136 | for the __98th Percentile__, the Mean = 26.62 and Variance = 180.43*\n",
    "\n",
    "*For **PM10** __2nd Max__, the Mean = 67.87 and Var = 11427.30*\n",
    "\n",
    "*For **O3** __4th max__, the mean = 0.0718 and the var = 0.0001*\n",
    "\n",
    "*For **CO** __2nd Max__, the mean =1.91 and var =1.05*\n",
    "\n",
    "*For **SO2** __99th percentile__, the mean =48.94 and var = 2703.83*\n",
    "\n",
    "*For **NO2** __Annual Mean__, the mean = 10.46 and var = 29.78 | for the __98th percentile__, the mean =45.03 and var = 172.75*\n",
    "\n",
    "*For **Pb** __max 3-month Average__, the Mean = 0.186 and Var = 0.159.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II\n",
    "\n",
    "Focus on the PM2.5 measurements that are non-missing most of the time. Answer each of the following questions in a brief paragraph or two. Do not describe your analyses step-by-step for your answers; instead, report your findings. Your paragraph(s) should indicate both your answer to the question and a justification for your answer; _**please do not include codes with your answers**_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Has PM 2.5 air pollution improved in the U.S. on the whole since 2000?\n",
    "\n",
    "PM2.5 air pollution in the U.S. has improved on the whole since 2000 to 2019. The weighted annual mean across the U.S. has decreased by an average of 5.498 which means that the average daily concentration of PM2.5 in the air has decreased by 5.498.\n",
    "This result was taken from the average change of 2019 weighted annual means and the 2000 weighted annual means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Over time, has PM 2.5 pollution become more variable, less variable, or about equally variable from city to city in the U.S.?\n",
    "\n",
    "Over time, PM 2.5 pollution has become less variable for around 125 cities meaning that observations of PM2.5 pollution are more consistant. For the other 89 cities, the variability increased. Nevertheless, the average variability decreased over time from all cities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which state has seen the greatest improvement in PM 2.5 pollution over time? Which city has seen the greatest improvement?\n",
    "\n",
    "Alabama is the state that had the greatest improvement in PM2.5 pollution and Portsmouth, OH is the city that had the greatest improvement. On average, compared to other states and cities respectively, these places had the largest decrease in weighted annual mean of PM2.5 pollution from 2000 to 2019. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose a location with some meaning to you (e.g. hometown, family lives there, took a vacation there, etc.). Was that location in compliance with EPA primary standards as of the most recent measurement?\n",
    "\n",
    "I chose to compare Urban Honolulu, Hawaii to the EPA primary standards. With a search of the EPA standards, and comparing it do my data, I see that Hawaii was in compliance with the EPA primary Standards because it did not exceed any acceptable pollutant levels in 2019."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation\n",
    "\n",
    "One strategy for filling in missing values ('imputation') is to use non-missing values to predict the missing ones; the success of this strategy depends in part on the strength of relationship between the variable(s) used as predictors of missing values. \n",
    "\n",
    "Identify one other pollutant that might be a good candidate for imputation based on the PM 2.5 measurements and explain why you selected the variable you did. Can you envision any potential pitfalls to this technique?\n",
    "\n",
    "Another pollutant that might be a good candidate for imputation based on the PM2.5 measurements is NO2 annual mean because the averages across the states per year are very similar to each other. The averages have a tendency to increase together and decrease together with exceptions. Some Pitfalls for this technique are that it could maybe add some bias into the data. Or that there is a chance that NO2 coincedentally had a correlation with the PM2.5 but not actually a relationship which could cause inaccurate inputation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.057944</td>\n",
       "      <td>12.688318</td>\n",
       "      <td>12.352336</td>\n",
       "      <td>11.853271</td>\n",
       "      <td>11.642056</td>\n",
       "      <td>12.479439</td>\n",
       "      <td>11.360748</td>\n",
       "      <td>11.573364</td>\n",
       "      <td>10.625234</td>\n",
       "      <td>9.671028</td>\n",
       "      <td>9.830374</td>\n",
       "      <td>9.638318</td>\n",
       "      <td>8.973364</td>\n",
       "      <td>8.798598</td>\n",
       "      <td>8.660748</td>\n",
       "      <td>8.342523</td>\n",
       "      <td>7.585047</td>\n",
       "      <td>7.942991</td>\n",
       "      <td>8.115421</td>\n",
       "      <td>7.559813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85.745631</td>\n",
       "      <td>96.267961</td>\n",
       "      <td>83.975728</td>\n",
       "      <td>88.920388</td>\n",
       "      <td>75.337864</td>\n",
       "      <td>72.496117</td>\n",
       "      <td>76.801942</td>\n",
       "      <td>66.819417</td>\n",
       "      <td>66.979612</td>\n",
       "      <td>56.411650</td>\n",
       "      <td>61.678641</td>\n",
       "      <td>61.904854</td>\n",
       "      <td>63.445631</td>\n",
       "      <td>59.825243</td>\n",
       "      <td>56.022330</td>\n",
       "      <td>54.106796</td>\n",
       "      <td>51.823301</td>\n",
       "      <td>63.598058</td>\n",
       "      <td>62.761165</td>\n",
       "      <td>52.560194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.079849</td>\n",
       "      <td>0.080676</td>\n",
       "      <td>0.084433</td>\n",
       "      <td>0.079331</td>\n",
       "      <td>0.072570</td>\n",
       "      <td>0.077553</td>\n",
       "      <td>0.075451</td>\n",
       "      <td>0.076373</td>\n",
       "      <td>0.071704</td>\n",
       "      <td>0.066718</td>\n",
       "      <td>0.070268</td>\n",
       "      <td>0.070525</td>\n",
       "      <td>0.072722</td>\n",
       "      <td>0.065215</td>\n",
       "      <td>0.065060</td>\n",
       "      <td>0.065651</td>\n",
       "      <td>0.066232</td>\n",
       "      <td>0.065327</td>\n",
       "      <td>0.066856</td>\n",
       "      <td>0.062525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.310169</td>\n",
       "      <td>3.072881</td>\n",
       "      <td>2.801695</td>\n",
       "      <td>2.588136</td>\n",
       "      <td>2.416949</td>\n",
       "      <td>2.169492</td>\n",
       "      <td>2.150847</td>\n",
       "      <td>1.866102</td>\n",
       "      <td>1.750847</td>\n",
       "      <td>1.703390</td>\n",
       "      <td>1.610169</td>\n",
       "      <td>1.554237</td>\n",
       "      <td>1.540678</td>\n",
       "      <td>1.430508</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>1.408475</td>\n",
       "      <td>1.369492</td>\n",
       "      <td>1.379661</td>\n",
       "      <td>1.401695</td>\n",
       "      <td>1.233898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55.731343</td>\n",
       "      <td>55.164179</td>\n",
       "      <td>53.701493</td>\n",
       "      <td>52.194030</td>\n",
       "      <td>49.626866</td>\n",
       "      <td>49.746269</td>\n",
       "      <td>48.611940</td>\n",
       "      <td>47.313433</td>\n",
       "      <td>46.955224</td>\n",
       "      <td>43.313433</td>\n",
       "      <td>44.328358</td>\n",
       "      <td>43.194030</td>\n",
       "      <td>41.029851</td>\n",
       "      <td>40.492537</td>\n",
       "      <td>40.716418</td>\n",
       "      <td>39.044776</td>\n",
       "      <td>37.985075</td>\n",
       "      <td>37.582090</td>\n",
       "      <td>37.268657</td>\n",
       "      <td>36.656716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>78.168539</td>\n",
       "      <td>79.224719</td>\n",
       "      <td>72.921348</td>\n",
       "      <td>74.337079</td>\n",
       "      <td>72.337079</td>\n",
       "      <td>72.393258</td>\n",
       "      <td>70.022472</td>\n",
       "      <td>65.314607</td>\n",
       "      <td>55.831461</td>\n",
       "      <td>50.134831</td>\n",
       "      <td>46.876404</td>\n",
       "      <td>37.943820</td>\n",
       "      <td>38.483146</td>\n",
       "      <td>36.179775</td>\n",
       "      <td>33.067416</td>\n",
       "      <td>26.887640</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>17.337079</td>\n",
       "      <td>15.674157</td>\n",
       "      <td>14.719101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>34.130841</td>\n",
       "      <td>34.387850</td>\n",
       "      <td>33.397196</td>\n",
       "      <td>29.799065</td>\n",
       "      <td>31.962617</td>\n",
       "      <td>33.794393</td>\n",
       "      <td>29.116822</td>\n",
       "      <td>30.859813</td>\n",
       "      <td>27.373832</td>\n",
       "      <td>24.920561</td>\n",
       "      <td>24.803738</td>\n",
       "      <td>24.714953</td>\n",
       "      <td>22.074766</td>\n",
       "      <td>23.018692</td>\n",
       "      <td>22.261682</td>\n",
       "      <td>21.602804</td>\n",
       "      <td>19.294393</td>\n",
       "      <td>21.612150</td>\n",
       "      <td>23.813084</td>\n",
       "      <td>19.612150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14.393258</td>\n",
       "      <td>14.337079</td>\n",
       "      <td>13.955056</td>\n",
       "      <td>13.483146</td>\n",
       "      <td>12.640449</td>\n",
       "      <td>12.674157</td>\n",
       "      <td>12.044944</td>\n",
       "      <td>11.674157</td>\n",
       "      <td>10.853933</td>\n",
       "      <td>9.910112</td>\n",
       "      <td>9.640449</td>\n",
       "      <td>9.483146</td>\n",
       "      <td>9.101124</td>\n",
       "      <td>8.719101</td>\n",
       "      <td>8.438202</td>\n",
       "      <td>8.123596</td>\n",
       "      <td>7.707865</td>\n",
       "      <td>7.449438</td>\n",
       "      <td>7.471910</td>\n",
       "      <td>7.179775</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0          1          2          3          4          5   \\\n",
       "0  13.057944  12.688318  12.352336  11.853271  11.642056  12.479439   \n",
       "1  85.745631  96.267961  83.975728  88.920388  75.337864  72.496117   \n",
       "2   0.079849   0.080676   0.084433   0.079331   0.072570   0.077553   \n",
       "3   3.310169   3.072881   2.801695   2.588136   2.416949   2.169492   \n",
       "4  55.731343  55.164179  53.701493  52.194030  49.626866  49.746269   \n",
       "5  78.168539  79.224719  72.921348  74.337079  72.337079  72.393258   \n",
       "6  34.130841  34.387850  33.397196  29.799065  31.962617  33.794393   \n",
       "7  14.393258  14.337079  13.955056  13.483146  12.640449  12.674157   \n",
       "\n",
       "          6          7          8          9          10         11  \\\n",
       "0  11.360748  11.573364  10.625234   9.671028   9.830374   9.638318   \n",
       "1  76.801942  66.819417  66.979612  56.411650  61.678641  61.904854   \n",
       "2   0.075451   0.076373   0.071704   0.066718   0.070268   0.070525   \n",
       "3   2.150847   1.866102   1.750847   1.703390   1.610169   1.554237   \n",
       "4  48.611940  47.313433  46.955224  43.313433  44.328358  43.194030   \n",
       "5  70.022472  65.314607  55.831461  50.134831  46.876404  37.943820   \n",
       "6  29.116822  30.859813  27.373832  24.920561  24.803738  24.714953   \n",
       "7  12.044944  11.674157  10.853933   9.910112   9.640449   9.483146   \n",
       "\n",
       "          12         13         14         15         16         17  \\\n",
       "0   8.973364   8.798598   8.660748   8.342523   7.585047   7.942991   \n",
       "1  63.445631  59.825243  56.022330  54.106796  51.823301  63.598058   \n",
       "2   0.072722   0.065215   0.065060   0.065651   0.066232   0.065327   \n",
       "3   1.540678   1.430508   1.400000   1.408475   1.369492   1.379661   \n",
       "4  41.029851  40.492537  40.716418  39.044776  37.985075  37.582090   \n",
       "5  38.483146  36.179775  33.067416  26.887640  21.000000  17.337079   \n",
       "6  22.074766  23.018692  22.261682  21.602804  19.294393  21.612150   \n",
       "7   9.101124   8.719101   8.438202   8.123596   7.707865   7.449438   \n",
       "\n",
       "          18         19  \n",
       "0   8.115421   7.559813  \n",
       "1  62.761165  52.560194  \n",
       "2   0.066856   0.062525  \n",
       "3   1.401695   1.233898  \n",
       "4  37.268657  36.656716  \n",
       "5  15.674157  14.719101  \n",
       "6  23.813084  19.612150  \n",
       "7   7.471910   7.179775  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# raw data\n",
    "air_raw = pd.read_csv('data/air-quality.csv')\n",
    "cbsa_info = pd.read_csv('data/cbsa-info.csv')\n",
    "\n",
    "\n",
    "## PART I\n",
    "dm = pd.merge(cbsa_info, air_raw, how = 'left', on = 'CBSA')\n",
    "\n",
    "\n",
    "dm.nunique()\n",
    "\n",
    "#splitting the city/state names into just the 2 letter state codes\n",
    "areas = dm['Core Based Statistical Area']\n",
    "\n",
    "state_1 = []\n",
    "for i in range(len(areas)):\n",
    "    state_1.append((areas[i].split(', ')[1]).split('-'))\n",
    "\n",
    "state_2 = []\n",
    "for i in range(len(state_1)):\n",
    "    if len(state_1[i]) > 1:\n",
    "        for f in range(len(state_1[i])):\n",
    "            state_2.append(state_1[i][f])\n",
    "    else:\n",
    "        state_2.append(state_1[i][0])\n",
    "len(set(state_2))\n",
    "len(dm)\n",
    "\n",
    "#tidying the data\n",
    "dm1 = dm.melt(\n",
    "    #ignore_index = False,\n",
    "    id_vars = ['CBSA', 'Core Based Statistical Area', 'Trend Statistic', 'Number of Trends Sites', 'Pollutant'],\n",
    "    var_name = 'Year',\n",
    "    value_name = 'quantity'\n",
    ")\n",
    "\n",
    "dm2 = dm1.pivot(\n",
    "    columns = 'Pollutant',\n",
    "    values = 'quantity'\n",
    ")\n",
    "\n",
    "#assigning the tidy data to a new variable\n",
    "result = dm1.merge(dm2, left_index=True, right_index=True)\n",
    "result.drop(columns = ['Pollutant', 'quantity'], inplace = True)\n",
    "#result.isna().sum()/len(result)\n",
    "result\n",
    "\n",
    "#checking the percentages of missing values from the data\n",
    "len(dm[dm[\"Pollutant\"] == \"PM2.5\"][[\"CBSA\", \"Pollutant\"]].drop_duplicates())\n",
    "# 214 CBSA's with pm2.5 measurements with a total of 351 CBSA\n",
    "len(dm[dm[\"Pollutant\"] == \"PM10\"][[\"CBSA\", \"Pollutant\"]].drop_duplicates())\n",
    "#103 CBSA's with pm10 measurements\n",
    "len(dm[dm[\"Pollutant\"] == \"CO\"][[\"CBSA\", \"Pollutant\"]].drop_duplicates())\n",
    "#51 / 351\n",
    "len(dm[dm[\"Pollutant\"] == \"O3\"][[\"CBSA\", \"Pollutant\"]].drop_duplicates())\n",
    "# 284 / 351\n",
    "len(dm[dm[\"Pollutant\"] == \"SO2\"][[\"CBSA\", \"Pollutant\"]].drop_duplicates())\n",
    "# 89 / 351\n",
    "len(dm[dm[\"Pollutant\"] == \"NO2\"][[\"CBSA\", \"Pollutant\"]].drop_duplicates())\n",
    "#89 / 351\n",
    "len(dm[dm[\"Pollutant\"] == \"Pb\"][[\"CBSA\", \"Pollutant\"]].drop_duplicates())\n",
    "#15 / 351\n",
    "\n",
    "#checking the means between all CBSA of each pollutant\n",
    "trends = result[\"Trend Statistic\"].unique()\n",
    "result[result[\"Trend Statistic\"] == \"Weighted Annual Mean\"][\"PM2.5\"].mean() # PM2.5 mean 10.137546728971962  *\n",
    "result[result[\"Trend Statistic\"] == trends[0]][\"PM10\"].mean() # 2nd max pm10 67.874\n",
    "result[result[\"Trend Statistic\"] == trends[0]][\"CO\"].mean() # 2nd max CO 1.9079\n",
    "result[result[\"Trend Statistic\"] == trends[2]][\"PM2.5\"].mean() # 98th Percentile PM2.5 26.627570093457944  *\n",
    "result[result[\"Trend Statistic\"] == trends[2]][\"NO2\"].mean() # 98th percentile NO2 45.03283582089552\n",
    "result[result[\"Trend Statistic\"] == trends[3]][\"O3\"].mean() # 2nd max O3 0.07175193661971829\n",
    "result[result[\"Trend Statistic\"] == trends[4]][\"SO2\"].mean() # 99th percentile SO2 48.942696629213486\n",
    "result[result[\"Trend Statistic\"] == trends[5]][\"NO2\"].mean() # Annual Mean SO2 10.464044943820225\n",
    "result[result[\"Trend Statistic\"] == trends[6]][\"Pb\"].mean() # Max 3-month average Pb 0.18646666666666667\n",
    "\n",
    "# checking the variance between all CBSA and each pollutant\n",
    "result[result[\"Trend Statistic\"] == trends[1]][\"PM2.5\"].var(ddof=1) # variance of PM2.5 10.13568035537607\n",
    "result[result[\"Trend Statistic\"] == trends[0]][\"PM10\"].var() # 11427.296187919954\n",
    "result[result[\"Trend Statistic\"] == trends[0]][\"CO\"].var() # 1.051912738459769\n",
    "result[result[\"Trend Statistic\"] == trends[2]][\"PM2.5\"].var() # 180.43569682845802\n",
    "result[result[\"Trend Statistic\"] == trends[2]][\"NO2\"].var() # 172.75172160110577\n",
    "result[result[\"Trend Statistic\"] == trends[3]][\"O3\"].var() #0.0001069194362911046\n",
    "result[result[\"Trend Statistic\"] == trends[4]][\"SO2\"].var() # 2703.837074230568\n",
    "result[result[\"Trend Statistic\"] == trends[5]][\"NO2\"].var() # 29.780044337495497\n",
    "result[result[\"Trend Statistic\"] == trends[6]][\"Pb\"].var() # 0.15972325975473803\n",
    "\n",
    "#dm[\"Pollutant\"].unique()\n",
    "\n",
    "\n",
    "## PART II\n",
    "##########\n",
    "\n",
    "\n",
    "\n",
    "# Take the data and only look at PM2.5 weighted annual mean from 2000 to 2019\n",
    "dm_mean = dm[(dm['Pollutant'] == 'PM2.5') & (dm['Trend Statistic'] == 'Weighted Annual Mean')][[\"CBSA\",\"Pollutant\",\"Trend Statistic\", \"2000\", '2019']] \n",
    "dm_mean['2019'].mean() - dm_mean['2000'].mean()\n",
    "\n",
    "\n",
    "# Take the variance of the different years \n",
    "dm_var = dm[(dm['Pollutant'] == 'PM2.5') & (dm['Trend Statistic'] == 'Weighted Annual Mean')]\n",
    "dm_var = dm_var.drop('Number of Trends Sites', axis=1)\n",
    "#dm_var.iloc[:, [1] + list(range(4, 24))]\n",
    "#dm_var.iloc[0, 4:24]\n",
    "dm_var = dm_var.reset_index()\n",
    "\n",
    "#separating the variance in 5 year groups 2000-2005, 2006-2010, etc.\n",
    "cityvar2 = {}\n",
    "cityvar3 = {}\n",
    "cityvar4 = {}\n",
    "cityvar5 = {}\n",
    "\n",
    "for x in range(214):\n",
    "    row_data = dm_var.iloc[x, 5:10]\n",
    "    cityvar2[str(dm_var.loc[x, 'Core Based Statistical Area'])] = row_data.var()\n",
    "\n",
    "for x in range(214):\n",
    "    row_data = dm_var.iloc[x, 10:15]\n",
    "    cityvar3[str(dm_var.loc[x, 'Core Based Statistical Area'])] = row_data.var()\n",
    "\n",
    "for x in range(214):\n",
    "    row_data = dm_var.iloc[x, 15:20]\n",
    "    cityvar4[str(dm_var.loc[x, 'Core Based Statistical Area'])] = row_data.var()\n",
    "\n",
    "for x in range(214):\n",
    "    row_data = dm_var.iloc[x, 20:25]\n",
    "    cityvar5[str(dm_var.loc[x, 'Core Based Statistical Area'])] = row_data.var()\n",
    "\n",
    "cityvar2\n",
    "cityvar3\n",
    "cityvar4\n",
    "cityvar5\n",
    "\n",
    "citydata = [\n",
    "    cityvar2, cityvar3, cityvar4, cityvar5\n",
    "]\n",
    "\n",
    "# Create the PM2.5 Variance Data Frame (Columns are 5 year groups, Rows are cities)\n",
    "df = pd.DataFrame(citydata)\n",
    "df.index = [\"2000-2004\", \"2005-2009\", \"2010-2014\", \"2015-2019\"]\n",
    "dft = df.transpose()\n",
    "dft\n",
    "\n",
    "#Look at the average change in variability between years\n",
    "\n",
    "dft[\"Average Variability Change\"] = (dft[\"2005-2009\"] - dft[\"2000-2004\"]) + (dft[\"2010-2014\"] - dft[\"2005-2009\"]) + (dft[\"2015-2019\"] - dft[\"2010-2014\"])\n",
    "dft\n",
    "\n",
    "#This is the point where i realize i didnt need to do 2005-2009 and 2010-2014 for the way I am doing this\n",
    "\n",
    "#for the sake of the question because I don't want to list every city, I will count how many variabilities increased and how many decreased and how many stayed the same\n",
    "dftplus = dft[dft['Average Variability Change'] > 0]\n",
    "len(dftplus)\n",
    "#variability / variance increased for 89 CBSA's\n",
    "\n",
    "dftmin = dft[dft['Average Variability Change'] < 0]\n",
    "len(dftmin)\n",
    "# decreased for 125\n",
    "\n",
    "dfteq = dft[dft['Average Variability Change'] == 0]\n",
    "len(dfteq)\n",
    "# stayed the same for 0\n",
    "\n",
    "# what is the average variablility from all cities together\n",
    "dft['Average Variability Change'].mean()\n",
    "\n",
    "\n",
    "#to look at the greatest city improvement, i will use the dm_mean where it shows the CBSA's means\n",
    "# make a new column with the changes\n",
    "dm_mean[\"Change\"] = dm_mean['2019'] - dm_mean['2000']\n",
    "\n",
    "# check which one has the highest negative change (means that pollution went down the most)\n",
    "dm_mean[\"Change\"].idxmin()\n",
    "#use the index to find the CBSA\n",
    "dm_mean.loc[790]\n",
    "# use CBSA to find name of the city\n",
    "cbsa_info[cbsa_info['CBSA'] == 39020]\n",
    "\n",
    "# take average of states\n",
    "set(state_2)\n",
    "state_mean = pd.merge(dm_mean, cbsa_info, how = 'left', on = 'CBSA')\n",
    "state_mean\n",
    "\n",
    "# make a new column of just state names\n",
    "states = []\n",
    "\n",
    "for state in state_mean['Core Based Statistical Area']: \n",
    "    states.append(state.split(' ')[-1])\n",
    "states\n",
    "state_mean[\"State\"] = states\n",
    "\n",
    "#calculate the average between states\n",
    "statemeans = {}\n",
    "\n",
    "for state in set(state_2):\n",
    "    change = 0\n",
    "    count = 0\n",
    "    bool = state_mean['State'].str.contains(state)\n",
    "    for i in range(len(bool)):\n",
    "        if bool[i]:\n",
    "            change += state_mean[\"Change\"][i]\n",
    "            count += 1\n",
    "    if count > 0:    \n",
    "        statemeans[state] = change/count\n",
    "            \n",
    "statemeans\n",
    "min(statemeans, key=statemeans.get)\n",
    "\n",
    "\n",
    "# 46520 Hawaii\n",
    "# just show the most recent measurements of Hawaii \n",
    "dm[dm['CBSA'] == 46520].drop(dm.columns[5:24], axis=1)   \n",
    "\n",
    "\n",
    "#imputation\n",
    "\n",
    "list = []\n",
    "list1 = []\n",
    "list2 = []\n",
    "list3 = []\n",
    "list4 = []\n",
    "list5 = []\n",
    "list6 = []\n",
    "list7 = []\n",
    "\n",
    "for x in range(5, 25):\n",
    "    list.append(dm[(dm['Pollutant'] == 'PM2.5') & (dm['Trend Statistic'] == 'Weighted Annual Mean')].iloc[:, x].mean())\n",
    "for x in range(5, 25):\n",
    "    list1.append(dm[dm['Pollutant'] == 'PM10'].iloc[:, x].mean())\n",
    "for x in range(5, 25):\n",
    "    list2.append(dm[dm['Pollutant'] == 'O3'].iloc[:, x].mean())\n",
    "for x in range(5, 25):\n",
    "    list3.append(dm[dm['Pollutant'] == 'CO'].iloc[:, x].mean())\n",
    "for x in range(5, 25):\n",
    "    list4.append(dm[(dm['Pollutant'] == 'NO2') & (dm['Trend Statistic'] == '98th Percentile')].iloc[:, x].mean())\n",
    "for x in range(5, 25):\n",
    "    list5.append(dm[dm['Pollutant'] == 'SO2'].iloc[:, x].mean())\n",
    "\n",
    "for x in range(5, 25):\n",
    "    list6.append(dm[(dm['Pollutant'] == 'PM2.5') & (dm['Trend Statistic'] == '98th Percentile')].iloc[:, x].mean())\n",
    "for x in range(5, 25):\n",
    "    list7.append(dm[(dm['Pollutant'] == 'NO2') & (dm['Trend Statistic'] == 'Annual Mean')].iloc[:, x].mean())\n",
    "\n",
    "data = [list,\n",
    "list1,\n",
    "list2,\n",
    "list3,\n",
    "list4,\n",
    "list5,\n",
    "list6,\n",
    "list7\n",
    "       ]\n",
    "\n",
    "#checking the correlation\n",
    "asdf = pd.DataFrame(data)\n",
    "asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AR': -6.0,\n",
       " 'MA': -5.875,\n",
       " 'FL': -4.158333333333333,\n",
       " 'NE': -3.65,\n",
       " 'PA': -6.46923076923077,\n",
       " 'MT': -8.2,\n",
       " 'NV': -3.1000000000000005,\n",
       " 'OK': -2.8000000000000007,\n",
       " 'PR': -1.7000000000000002,\n",
       " 'MI': -4.959999999999999,\n",
       " 'CT': -5.199999999999999,\n",
       " 'IN': -7.1,\n",
       " 'TX': -3.0599999999999996,\n",
       " 'VT': -3.5999999999999996,\n",
       " 'KY': -8.144444444444446,\n",
       " 'AZ': -3.575,\n",
       " 'GA': -7.8090909090909095,\n",
       " 'OH': -8.064285714285715,\n",
       " 'WA': -3.0000000000000004,\n",
       " 'UT': -4.166666666666667,\n",
       " 'CA': -5.222727272727273,\n",
       " 'SC': -7.225,\n",
       " 'MN': -3.3800000000000003,\n",
       " 'NC': -7.955555555555555,\n",
       " 'NY': -5.720000000000001,\n",
       " 'MO': -5.449999999999999,\n",
       " 'WY': -3.5333333333333328,\n",
       " 'DE': -6.666666666666667,\n",
       " 'CO': -2.0750000000000006,\n",
       " 'WI': -4.2875000000000005,\n",
       " 'IA': -3.9499999999999997,\n",
       " 'KS': -4.15,\n",
       " 'MD': -7.420000000000002,\n",
       " 'SD': -3.25,\n",
       " 'TN': -8.800000000000002,\n",
       " 'DC': -7.2,\n",
       " 'VA': -7.875000000000001,\n",
       " 'IL': -6.166666666666667,\n",
       " 'AK': -1.7,\n",
       " 'LA': -5.683333333333334,\n",
       " 'ND': -1.5,\n",
       " 'WV': -8.3,\n",
       " 'NM': -0.9999999999999996,\n",
       " 'HI': -1.15,\n",
       " 'RI': -5.6000000000000005,\n",
       " 'NH': -5.433333333333334,\n",
       " 'NJ': -6.42,\n",
       " 'OR': -0.37500000000000044,\n",
       " 'AL': -8.888888888888891}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statemeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Notes on merging (keep at bottom of notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To combine datasets based on shared information, you can use the `pd.merge(A, B, how = ..., on = SHARED_COLS)` function, which will match the rows of `A` and `B` based on the shared columns `SHARED_COLS`. If `how = 'left'`, then only rows in `A` will be retained in the output (so `B` will be merged *to* `A`); conversely, if `how = 'right'`, then only rows in `B` will be retained in the output (so `A` will be merged *to* `B`).\n",
    "\n",
    "A simple example of the use of `pd.merge` is illustrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy data frames\n",
    "A = pd.DataFrame(\n",
    "    {'shared_col': ['a', 'b', 'c'], \n",
    "    'x1': [1, 2, 3], \n",
    "    'x2': [4, 5, 6]}\n",
    ")\n",
    "\n",
    "B = pd.DataFrame(\n",
    "    {'shared_col': ['a', 'b'], \n",
    "    'y1': [7, 8]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, if `A` and `B` are merged retaining the rows in `A`, notice that a missing value is input because `B` has no row where the shared column (on which the merging is done) has value `c`. In other words, the third row of `A` has no match in `B`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# left join\n",
    "pd.merge(A, B, how = 'left', on = 'shared_col')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the direction of merging is reversed, and the row structure of `B` is dominant, then the third row of `A` is dropped altogether because it has no match in `B`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# right join\n",
    "pd.merge(A, B, how = 'right', on = 'shared_col')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
